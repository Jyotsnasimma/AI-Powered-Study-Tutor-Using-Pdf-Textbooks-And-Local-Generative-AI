{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42eb255-06bb-4c6b-89d3-c4d935521f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad1caa0-797a-45f2-8fc3-5c8e6fb0ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.3.67)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (0.4.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (23.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyttsx3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (2.98)\n",
      "Requirement already satisfied: comtypes in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from pyttsx3) (1.4.11)\n",
      "Requirement already satisfied: pypiwin32 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyttsx3) (305.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.53.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install faiss-cpu\n",
    "!pip install pyttsx3\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install sentence-transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b168ce5b-97b5-44a9-9656-edd40ff20a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (1.26.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a130728d-0f33-4bab-ae50-d1000fc3bc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (3.14.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from SpeechRecognition) (4.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyaudio in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (0.2.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fe4d84-652f-49d6-947b-597baf28ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 84972\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def load_pdf(path):\n",
    "    doc = fitz.open(\"DSA Full Notes GR-20.pdf\")\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"DSA Full Notes GR-20.pdf\"  # Make sure this file is in the same folder as your notebook\n",
    "raw_text = load_pdf(pdf_path)\n",
    "print(\"Text length:\", len(raw_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba74eab1-1d5b-46ac-ad10-e715fada0a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (5.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.53.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (0.33.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\simma_4uy3cu0\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d957b969-b7bd-4dc4-a5b6-6cae064f8286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 213\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define your chunking method\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Each chunk will have ~500 characters\n",
    "    chunk_overlap=100  # Overlap between chunks (helps for continuity)\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6d786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Extracting PDF text...\n",
      "üîÑ Splitting into chunks...\n",
      "‚úÖ PDF split into 26 chunks.\n",
      "üîç Creating FAISS index...\n",
      "‚úÖ Ready! Ask any question (type 'exit' to quit).\n",
      "üîç Searching PDF for relevant content...\n",
      "ü§ñ Generating answer...\n",
      "\n",
      "üí¨ AI Tutor Answer:\n",
      "Datascienc√© refers to the quality of having sufficient information or a large amount of data for a specific task or purpose. In this context, it can be used as an interrogative, indicating whether or not there is a significant amount of relevant information available for the given question. For example:\n",
      "\n",
      "Question: What are the datasciec√©s of prerequisites for logistics management?\n",
      "\n",
      "Answer: There are many datasciec√©s available regarding the required prerequisites for logistics management.\n",
      "\n",
      "üîä Speaking answer...\n",
      "üëã Exiting AI Tutor. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pyttsx3\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Extract text from PDF\n",
    "# ------------------------------\n",
    "def extract_pdf_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Split text into chunks\n",
    "# ------------------------------\n",
    "def split_into_chunks(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Create FAISS index\n",
    "# ------------------------------\n",
    "def create_faiss_index(chunks, model):\n",
    "    embeddings = model.encode(chunks)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, embeddings\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Find top N best chunks\n",
    "# ------------------------------\n",
    "def find_top_chunks(question, chunks, model, index, top_n=3):\n",
    "    q_embedding = model.encode([question])\n",
    "    distances, indices = index.search(np.array(q_embedding), k=top_n)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Ask TinyLLaMA using Ollama\n",
    "# ------------------------------\n",
    "def ask_local_llm(context, question):\n",
    "    prompt = f\"\"\"You are a helpful AI tutor. Based only on the textbook content below, answer the question clearly and accurately.\n",
    "\n",
    "Text:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Speak the answer (TTS)\n",
    "# ------------------------------\n",
    "def speak_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN SCRIPT (Interactive Chat with Voice)\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"DSA Full Notes GR-20.pdf\"  # <-- Your PDF file path\n",
    "\n",
    "    print(\"üìñ Extracting PDF text...\")\n",
    "    full_text = extract_pdf_text(pdf_path)\n",
    "    if not full_text.strip():\n",
    "        print(\"‚ùå Could not extract text from the PDF.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"üîÑ Splitting into chunks...\")\n",
    "    chunks = split_into_chunks(full_text)\n",
    "    print(f\"‚úÖ PDF split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"üîç Creating FAISS index...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    index, embeddings = create_faiss_index(chunks, model)\n",
    "    print(\"‚úÖ Ready! Ask any question (type 'exit' to quit).\")\n",
    "\n",
    "    # Interactive Loop\n",
    "    while True:\n",
    "        question = input(\"\\n‚ùì Your Question: \")\n",
    "        if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"üëã Exiting AI Tutor. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"üîç Searching PDF for relevant content...\")\n",
    "        top_chunks = find_top_chunks(question, chunks, model, index, top_n=3)\n",
    "        combined_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "        print(\"ü§ñ Generating answer...\\n\")\n",
    "        answer = ask_local_llm(combined_context, question)\n",
    "\n",
    "        print(f\"üí¨ AI Tutor Answer:\\n{answer}\\n\")\n",
    "\n",
    "        # Speak answer\n",
    "        print(\"üîä Speaking answer...\")\n",
    "        speak_text(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d184731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Extracting PDF text...\n",
      "üîÑ Splitting into chunks...\n",
      "‚úÖ PDF split into 26 chunks.\n",
      "üîç Creating FAISS index...\n",
      "‚úÖ Ready! Say 'exit' to quit.\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùå Sorry, I couldn't understand. Please try again.\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùì You asked: what is data\n",
      "üîç Searching PDF for relevant content...\n",
      "ü§ñ Generating answer...\n",
      "\n",
      "üí¨ AI Tutor Answer:\n",
      "The given text provides information on a question regarding data, which includes two parts. The first part asks what is the given dataset and the second part provides an answer, which states that the given dataset consists of training data (features) to be used for fitting with a Linear Regression Model (when Degradation Level = 2).\n",
      "\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùå Sorry, I couldn't understand. Please try again.\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùå Sorry, I couldn't understand. Please try again.\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùå Sorry, I couldn't understand. Please try again.\n",
      "üé§ Listening... Please speak your question.\n",
      "‚ùì You asked: exit\n",
      "üëã Exiting AI Tutor. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Extract text from PDF\n",
    "# ------------------------------\n",
    "def extract_pdf_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Split text into chunks\n",
    "# ------------------------------\n",
    "def split_into_chunks(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Create FAISS index\n",
    "# ------------------------------\n",
    "def create_faiss_index(chunks, model):\n",
    "    embeddings = model.encode(chunks)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, embeddings\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Find top N best chunks\n",
    "# ------------------------------\n",
    "def find_top_chunks(question, chunks, model, index, top_n=3):\n",
    "    q_embedding = model.encode([question])\n",
    "    distances, indices = index.search(np.array(q_embedding), k=top_n)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Ask TinyLLaMA using Ollama\n",
    "# ------------------------------\n",
    "def ask_local_llm(context, question):\n",
    "    prompt = f\"\"\"You are a helpful AI tutor. Based only on the textbook content below, answer the question clearly and accurately.\n",
    "\n",
    "Text:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Speak the answer (TTS)\n",
    "# ------------------------------\n",
    "def speak_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Listen for voice question\n",
    "# ------------------------------\n",
    "def listen_question():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"üé§ Listening... Please speak your question.\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        question = recognizer.recognize_google(audio)\n",
    "        print(f\"‚ùì You asked: {question}\")\n",
    "        return question\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"‚ùå Sorry, I couldn't understand. Please try again.\")\n",
    "        return None\n",
    "    except sr.RequestError:\n",
    "        print(\"‚ùå Voice recognition service is unavailable.\")\n",
    "        return None\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN SCRIPT\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"DSA Full Notes GR-20.pdf\"  # <-- Your PDF file\n",
    "\n",
    "    print(\"üìñ Extracting PDF text...\")\n",
    "    full_text = extract_pdf_text(pdf_path)\n",
    "    if not full_text.strip():\n",
    "        print(\"‚ùå Could not extract text from the PDF.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"üîÑ Splitting into chunks...\")\n",
    "    chunks = split_into_chunks(full_text)\n",
    "    print(f\"‚úÖ PDF split into {len(chunks)} chunks.\")\n",
    "\n",
    "    print(\"üîç Creating FAISS index...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    index, embeddings = create_faiss_index(chunks, model)\n",
    "    print(\"‚úÖ Ready! Say 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        question = listen_question()\n",
    "        if not question:\n",
    "            continue\n",
    "        if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"üëã Exiting AI Tutor. Goodbye!\")\n",
    "            speak_text(\"Goodbye! Exiting AI Tutor.\")\n",
    "            break\n",
    "\n",
    "        print(\"üîç Searching PDF for relevant content...\")\n",
    "        top_chunks = find_top_chunks(question, chunks, model, index, top_n=3)\n",
    "        combined_context = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "        print(\"ü§ñ Generating answer...\\n\")\n",
    "        answer = ask_local_llm(combined_context, question)\n",
    "\n",
    "        print(f\"üí¨ AI Tutor Answer:\\n{answer}\\n\")\n",
    "        speak_text(answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa786060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What is predictive analysis?\n",
      "AI Answer: Predictive Analytics is a decision-making tool that uses statistical methods and machine learning algorithms to analyze historical data and make predictions about future outcomes. It is used in various industries, including finance, healthcare, e-commerce, and retail, among others, for forecasting, risk management, customer behavior analytics, fraud detection, etc. The goal of predictive analytics is to optimize decision making by using insights from data analysis to make informed decisions based on historical patterns, trends, and behaviors.\n",
      "Expected: Predictive analysis uses historical data and statistical models to predict future outcomes.\n",
      "Similarity Score: 0.74\n",
      "\n",
      "Q2: What is regression?\n",
      "AI Answer: Regresi√≥n es una t√©cnica para predecir o predictar valores de una variable en un conjunto de datos. Se basa en la interpretaci√≥n del comportamiento de la funci√≥n (o gr√°fico) logistico, que se cre√≥ para estimar una relaci√≥n lineal entre variables no lineales. La regresi√≥n consiste en encontrar las restricciones adecuadas en un modelolineario para asegurarse de que el modelo es adecuado y proporciona la mejor aproximaci√≥n posible del comportamiento del dep√≥sito de datos.\n",
      "Expected: Regression is a supervised learning technique used to predict continuous numerical values.\n",
      "Similarity Score: 0.34\n",
      "\n",
      "Q3: What is classification?\n",
      "AI Answer: Classification is a supervised Machine Learning technique used to predict categories, where the target variable (the output, which can be either a single value, vector of values or continuous outcome) can have only two possible values, and the classification model learns from a labelled dataset, where each observation has its own class/label. Based on this training data, then the model predicts the class for new/unsseen data. Classification models are used when there is a binary (yes/no), discrete or continuous outcome (e.g. Yes/No, sickness/health, disease/no disease) and are commonly applied in areas such as image classification, spam detection, prediction of stock prices, banking fraud detection, etc.\n",
      "Expected: Classification is a supervised learning task that assigns data into predefined labels.\n",
      "Similarity Score: 0.72\n",
      "\n",
      "Q4: What are the steps of data preprocessing?\n",
      "AI Answer: The steps of data preprocessing include:\n",
      "1. Data Collection: Collecting relevant and reliable data from various sources such as government agencies, social media platforms, and market research firms.\n",
      "2. Data Cleaning: Removing any irrelevant or noisy data to improve the accuracy and reliability of the data. This includes removing duplicates, converting non-standard values to standardized ones, and cleaning up text data by removing punctuations, capitalization, and other formatting errors.\n",
      "3. Data Enrichment: Adding additional data such as demographic information (such as age, gender, income level), past sales trends, customer preferences, or customer behavior patterns (such as online purchases, order history, and product feedback).\n",
      "4. Data Visualization: Previewing the data in various forms, such as graphs, charts, or tables, to better understand its structure and relationships.\n",
      "5. Data Normalization: Ensuring that all data elements are on a consistent scale (i.e., within a certain range), so that it can be analyzed and interpreted uniformly.\n",
      "6. Data Balancing: Addressing any imbalances or outliers in the data set, to ensure that each feature is represented equally.\n",
      "7. Feature Selection: Selecting the most relevant features based on the primary goals of the analysis, such as predictive model development, classification, regression, or clustering.\n",
      "8. Data Normalization: Ensuring that all data elements are on a consistent scale (i.e., within a certain range), so that it can be analyzed and interpreted uniformly.\n",
      "9. Data Preparation: Mapping the preprocessed data to the target variables (such as sales, revenue, or profit margins) for further analysis, modeling, or decision making.\n",
      "10. Modeling: Identifying and using appropriate statistical techniques (such as regression, classification, clustering, or dimensionality reduction) to extract useful insights from the preprocessed data.\n",
      "11. Model Evaluation: Performing the appropriate tests (such as hypothesis testing, cross-validation, or holdout evaluation) to determine if the model's predictions are accurate and reliable.\n",
      "12. Tuning Parameters: Adjusting the model's parameters (such as hyperparameters, regularization coefficients, and optimization methods) based on the results of the model evaluation and experimentation phase.\n",
      "13. Model Deployment: Putting the model into production or deployment in a business application or service for real-time or long-term usage.\n",
      "Expected: Data preprocessing involves data cleaning, normalization, transformation, and splitting.\n",
      "Similarity Score: 0.74\n",
      "\n",
      "Q5: What is clustering?\n",
      "AI Answer: Clustering is a technique used in data analysis to group similar objects or events together into groups based on certain criteria. It is commonly used to identify patterns and relationships within large datasets that would be difficult to discover using traditional statistical methods, such as correlation analysis or regression. In essence, clustering breaks down a dataset into smaller groups based on shared characteristics, allowing for more nuanced and granular insights into data.\n",
      "\n",
      "Clustering is usually performed using a range of techniques, including k-means, hierarchical clustering, or density estimation. These methods involve defining a number of categories (or clusters) based on the features and/or variables in the dataset, then selecting the most suitable groupings to represent these clusters. This process can be repeated many times until the data is grouped into desired levels or dimensions. Once the clusters are defined, they can then be analyzed for their unique characteristics, such as patterns, relationships, or correlations.\n",
      "\n",
      "In general, clustering has been used in a variety of applications, including market research, customer segmentation, and product recommendation systems. It is also commonly used in healthcare to identify patient clusters with similar medical histories or health outcomes. In finance, it is used for risk analysis and portfolio management, as well as in supply chain optimization.\n",
      "\n",
      "Clustering is a highly subjective technique that requires a deep understanding of the data being analyzed and an appropriate dataset to achieve reliable results. It can be a complex process, so it's always best to work with a qualified analyst or consultant to ensure maximum impact.\n",
      "Expected: Clustering is an unsupervised learning method used to group similar data points.\n",
      "Similarity Score: 0.73\n",
      "\n",
      "‚úÖ Final Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Prepare test Q&A pairs (you can add more questions)\n",
    "qa_pairs = [\n",
    "    (\"What is predictive analysis?\", \"Predictive analysis uses historical data and statistical models to predict future outcomes.\"),\n",
    "    (\"What is regression?\", \"Regression is a supervised learning technique used to predict continuous numerical values.\"),\n",
    "    (\"What is classification?\", \"Classification is a supervised learning task that assigns data into predefined labels.\"),\n",
    "    (\"What are the steps of data preprocessing?\", \"Data preprocessing involves data cleaning, normalization, transformation, and splitting.\"),\n",
    "    (\"What is clustering?\", \"Clustering is an unsupervised learning method used to group similar data points.\"),\n",
    "]\n",
    "\n",
    "# 2. Load sentence transformer for semantic similarity\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 3. Function to compute similarity\n",
    "def get_similarity(ans1, ans2):\n",
    "    emb1 = similarity_model.encode([ans1])\n",
    "    emb2 = similarity_model.encode([ans2])\n",
    "    return cosine_similarity(emb1, emb2)[0][0]\n",
    "\n",
    "# 4. Accuracy Test\n",
    "correct = 0\n",
    "for i, (question, expected) in enumerate(qa_pairs, 1):\n",
    "    # Use your AI tutor function to answer (replace 'ask_local_llm' with your function)\n",
    "    top_chunks = find_top_chunks(question, chunks, model, index, top_n=2)\n",
    "    context = \"\\n\".join(top_chunks)\n",
    "    ai_answer = ask_local_llm(context, question)  # Your existing code's answer\n",
    "    sim = get_similarity(ai_answer, expected)\n",
    "\n",
    "    print(f\"Q{i}: {question}\")\n",
    "    print(f\"AI Answer: {ai_answer}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Similarity Score: {sim:.2f}\\n\")\n",
    "\n",
    "    if sim > 0.7:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = (correct / len(qa_pairs)) * 100\n",
    "print(f\"‚úÖ Final Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
